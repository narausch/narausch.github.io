{"category":{"name":"Optimizations","normalized_name":"optimizations","subcategories":[{"name":"Compilers & Runtimes","normalized_name":"compilers-runtimes"}]},"foundation":"PyTorch","items":[{"category":"Optimizations","id":"optimizations--compilers-runtimes--depyf","name":"Depyf","logo":"logos/e4a063d36b8096ad076ac1c71881f1be4ccc0a18433cd7541d08de40f2c3370f.svg","subcategory":"Compilers & Runtimes","website":"https://depyf.readthedocs.io/en/latest/","description":"depyf is a tool to help you understand debug and get insights into pytorch.compile","primary_repository_url":"https://github.com/thuml/depyf"},{"category":"Optimizations","id":"optimizations--compilers-runtimes--glow","name":"Glow","logo":"logos/b3b7299a3f02acbea42cc57cc86165e6ca112e2729f9f7c3b5284ec36b263bf8.svg","subcategory":"Compilers & Runtimes","website":"https://github.com/pytorch/glow","description":"Glow is a machine learning compiler and execution engine for hardware accelerators. It is designed to be used as a backend for high-level machine learning frameworks. The compiler is designed to allow state of the art compiler optimizations and code generation of neural network graphs","primary_repository_url":"https://github.com/pytorch/glow"},{"category":"Optimizations","id":"optimizations--compilers-runtimes--intel-extension-for-pytorch","name":"intel-extension-for-pytorch","logo":"logos/149058a8b3c96359ac2f000c8958b4899897602728d52d917f9335d1e0273617.svg","subcategory":"Compilers & Runtimes","website":"https://intel.github.io/intel-extension-for-pytorch/","description":"A Python extension to optimize performance on an Intel platform.","primary_repository_url":"https://github.com/intel/intel-extension-for-pytorch"},{"category":"Optimizations","id":"optimizations--compilers-runtimes--octoml-profiler","name":"OctoML Profiler","logo":"logos/6a0430bf99ada8d6b9a2f7c296d8719bd6d5081f40036783c1ee51c58ae73217.svg","subcategory":"Compilers & Runtimes","website":"https://github.com/octoml/octoml-profile","description":"octoml-profile is a python library and cloud service that enables ML engineers to easily assess the performance and cost of PyTorch models on cloud hardware with state-of-the-art ML acceleration technology.","primary_repository_url":"https://github.com/octoml/octoml-profile"},{"category":"Optimizations","id":"optimizations--compilers-runtimes--onnx-runtime","name":"ONNX runtime","logo":"logos/a2ab7ad7ece050a1a7e0e667813b6992c1abc606d2112dd11c8c84836b5ba522.svg","subcategory":"Compilers & Runtimes","website":"https://onnxruntime.ai/","description":"High performance ML inferencing and training accelerator","primary_repository_url":"https://github.com/microsoft/onnxruntime"},{"category":"Optimizations","id":"optimizations--compilers-runtimes--poptorch","name":"PopTorch","logo":"logos/941ac64f6b2ec287a2dfc558c637747c9af0d2acdcf5e6148bdbfc5e2f8a81d5.svg","subcategory":"Compilers & Runtimes","website":"https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/","description":"PopTorch is a set of extensions for PyTorch enabling models to be trained, evaluated and used on the Graphcore IPU.","primary_repository_url":"https://github.com/graphcore/poptorch"},{"category":"Optimizations","id":"optimizations--compilers-runtimes--speedster","name":"Speedster","logo":"logos/ec9ec4b18b7c4f05d7caecffa57b81cf8639ba3442d8cbadba3b2bf53e806961.svg","subcategory":"Compilers & Runtimes","website":"https://www.nebuly.com/","description":"Speedster reduces inference costs by leveraging SOTA optimization techniques that best couple your AI models with the underlying hardware (GPUs and CPUs).","primary_repository_url":"https://github.com/nebuly-ai/optimate/tree/main/optimization/speedster"},{"category":"Optimizations","id":"optimizations--compilers-runtimes--torch-tensorrt","name":"Torch-TensorRT","logo":"logos/07dbeb39c385db2b246f1fb02d2534f65455facf03f6249afc3eaf34baaddfc1.svg","subcategory":"Compilers & Runtimes","website":"https://pytorch.org/TensorRT/","description":"Torch-TensorRT is a inference compiler for PyTorch, targeting NVIDIA GPUs via NVIDIAâ€™s TensorRT Deep Learning Optimizer and Runtime.","primary_repository_url":"https://github.com/RizhaoCai/PyTorch_ONNX_TensorRT"},{"category":"Optimizations","id":"optimizations--compilers-runtimes--pytorchxla","name":"PyTorchXLA","logo":"logos/56585018fdef58028329687f94025e78d6de2f303e1375427e95af5e2e878703.svg","subcategory":"Compilers & Runtimes","website":"https://pytorch.org/xla/release/2.1/index.html","description":"PyTorch runs on XLA devices, like TPUs, with the torch_xla package.","primary_repository_url":"https://github.com/pytorch/xla"}]}