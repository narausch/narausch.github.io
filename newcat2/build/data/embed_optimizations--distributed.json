{"category":{"name":"Optimizations","normalized_name":"optimizations","subcategories":[{"name":"Distributed","normalized_name":"distributed"}]},"foundation":"PyTorch","items":[{"category":"Optimizations","id":"optimizations--distributed--deepspeed","name":"DeepSpeed","logo":"logos/6a721f2f0c7a39901db79faf3d4307b17472c558462574a3ad699666f4d0e3f3.svg","subcategory":"Distributed","website":"https://www.deepspeed.ai/","description":"DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.","primary_repository_url":"https://github.com/microsoft/DeepSpeed"},{"category":"Optimizations","id":"optimizations--distributed--fairscale","name":"FairScale","logo":"logos/76b44191e2f21640c78f4d1afc3679f6bdefb6187441da1a84222ee5be3fb5f7.svg","subcategory":"Distributed","website":"https://github.com/facebookresearch/fairscale","description":"PyTorch extension library for high performance and large scale training. This library extends basic PyTorch capabilities while adding new SOTA scaling techniques.","primary_repository_url":"https://github.com/facebookresearch/fairscale"},{"category":"Optimizations","id":"optimizations--distributed--horovod","name":"Horovod","logo":"logos/a6f46dcbeb116393f35638875cc39a90f61ebc28453af34bf89ce3185da131ae.svg","subcategory":"Distributed","website":"http://horovod.ai/","description":"Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet","primary_repository_url":"https://github.com/horovod/horovod"}]}