{"category":{"name":"Training","normalized_name":"training","subcategories":[{"name":"General","normalized_name":"general"}]},"foundation":"PyTorch","items":[{"category":"Training","id":"training--general--adaptdl","name":"AdaptDL","logo":"logos/532512bdae901d661538ee38bb165d61918199ab2ba8f4d4cf0fddb2ed742647.svg","subcategory":"General","website":"https://adaptdl.readthedocs.io/en/latest/","description":"AdaptDL is a resource-adaptive deep learning (DL) training and scheduling framework, and is part of the CASL open source project.","primary_repository_url":"https://github.com/petuum/adaptdl"},{"category":"Training","id":"training--general--catalyst","name":"Catalyst","logo":"logos/b8f9b0d3ee7df42d688ddfaf95b3177f256d68521167ac27b601778e339fbedc.svg","subcategory":"General","website":"https://catalyst-team.com/","description":"Catalyst is a PyTorch framework for Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop.","primary_repository_url":"https://github.com/catalyst-team/catalyst"},{"category":"Training","id":"training--general--colossalai","name":"ColossalAI","logo":"logos/2125379104f8c98deea6e3dd3e0b2af88238792c2f41420624c1a142642c69ca.svg","subcategory":"General","website":"https://www.colossalai.org/","description":"Tools to kickstart distributed training and inference","primary_repository_url":"https://github.com/hpcaitech/ColossalAI"},{"category":"Training","id":"training--general--composer","name":"composer","logo":"logos/23206110f1d95753c27e1ebe4c63c6bc2c3b86060c4b332516cd9a3931f1900f.svg","subcategory":"General","website":"http://docs.mosaicml.com/","description":"A open-source deep learning training library optimized for scalability and usability, integrating best practices for efficient, multi-node training","primary_repository_url":"https://github.com/mosaicml/composer"},{"category":"Training","id":"training--general--fastai","name":"fastai","logo":"logos/9b1d9cefe662d9c4a595cbe44fddc8c2d31a2390f6ac3ac0ae025a8c878a53ce.svg","subcategory":"General","website":"https://docs.fast.ai/","description":"fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches","primary_repository_url":"https://github.com/fastai/fastai"},{"category":"Training","id":"training--general--hummingbird","name":"Hummingbird","logo":"logos/c0d704d4e08b297aadf925661feaf0dece433d3da38e551a9b0b0d326880447f.svg","subcategory":"General","website":"https://github.com/microsoft/hummingbird","description":"Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch) to accelerate traditional ML models.","primary_repository_url":"https://github.com/microsoft/hummingbird"},{"category":"Training","id":"training--general--ignite","name":"Ignite","logo":"logos/35bb1388cfcd432baba2f56afef9724cf9bbeca68724e9adcb4232272ad8309f.svg","subcategory":"General","website":"https://pytorch-ignite.ai/","description":"High-level library to help with training and evaluating neural networks in PyTorch","primary_repository_url":"https://github.com/pytorch/ignite"},{"category":"Training","id":"training--general--ludwig","name":"ludwig","logo":"logos/6e299a3f1f6df294a97064d136dd13a8078ec5ccc3d9cfdf4f3f57faf17e4b3b.svg","subcategory":"General","website":"https://ludwig.ai/latest/","description":"Ludwig is a low-code framework for building custom AI models like LLMs and other deep neural networks.","primary_repository_url":"https://github.com/ludwig-ai/ludwig"},{"category":"Training","id":"training--general--poutyne","name":"Poutyne","logo":"logos/a291fe1f4346bc027619c3baebe8eccb7043cece79f64e2582a80056699faa44.svg","subcategory":"General","website":"https://poutyne.org/","description":"Poutyne is a simplified framework for PyTorch and handles much of the boilerplating code needed to train neural networks.","primary_repository_url":"https://github.com/GRAAL-Research/poutyne"},{"category":"Training","id":"training--general--pytorch-lightning","name":"PyTorch Lightning","logo":"logos/3e3486bfb4e2fa782678a99e6da800be65f30c9c9f887ef4131bfe7634797187.svg","subcategory":"General","website":"https://lightning.ai/","description":"Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.","primary_repository_url":"https://github.com/Lightning-AI/pytorch-lightning"},{"category":"Training","id":"training--general--skorch","name":"skorch","logo":"logos/5fd4c8f1c65f2f5281e64ebe6b8e82bca7b16608042672804f452b8d8f2b1d98.svg","subcategory":"General","website":"https://github.com/skorch-dev/skorch","description":"A scikit-learn compatible neural network library that wraps PyTorch.","primary_repository_url":"https://github.com/skorch-dev/skorch"},{"category":"Training","id":"training--general--stoke","name":"stoke","logo":"logos/00076432de043a325ce5d921de4d7474db9c0f85e50fdc5f48ef3bc425417831.svg","subcategory":"General","website":"https://fidelity.github.io/stoke/","description":"A lightweight wrapper for PyTorch that provides a simple declarative API for context switching between devices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU with extensions (like optimizer state sharding) by simply changing a few declarative flags.","primary_repository_url":"https://github.com/StanfordPL/stoke"},{"category":"Training","id":"training--general--torchdistill","name":"torchdistill","logo":"logos/00e6cd8399a844be633526e835a2bff2a35c27c464134204177f96b484d0b393.svg","subcategory":"General","website":"https://yoshitomo-matsubara.net/torchdistill/","description":"Offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file.","primary_repository_url":"https://github.com/yoshitomo-matsubara/torchdistill"},{"category":"Training","id":"training--general--torchdata","name":"torchdata","logo":"logos/2d63d0eea40b76911bc64e1ea12a1520283429536287f6ed018c68dca29547b9.svg","subcategory":"General","website":"https://pytorch.org/data/beta/index.html","description":"A Beta library of common modular data loading primitives for easily constructing flexible and performant data pipelines.","primary_repository_url":"https://github.com/pytorch/data"},{"category":"Training","id":"training--general--torchmetrics","name":"TorchMetrics","logo":"logos/fe7ab2ceb03b0c3b20725bb4810500052a76b2285265de910812dfa0abbbdf6c.svg","subcategory":"General","website":"https://lightning.ai/docs/torchmetrics/","description":"Machine learning metrics for distributed, scalable PyTorch applications.","primary_repository_url":"https://github.com/Lightning-AI/torchmetrics"}]}