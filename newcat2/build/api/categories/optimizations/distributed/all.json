[{"category":"Optimizations","homepage_url":"https://www.deepspeed.ai/","id":"optimizations--distributed--deepspeed","logo_url":"http://127.0.0.1:8000/logos/6a721f2f0c7a39901db79faf3d4307b17472c558462574a3ad699666f4d0e3f3.svg","name":"DeepSpeed","subcategory":"Distributed","description":"DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.","repositories":[{"url":"https://github.com/microsoft/DeepSpeed","primary":true}]},{"category":"Optimizations","homepage_url":"https://github.com/facebookresearch/fairscale","id":"optimizations--distributed--fairscale","logo_url":"http://127.0.0.1:8000/logos/76b44191e2f21640c78f4d1afc3679f6bdefb6187441da1a84222ee5be3fb5f7.svg","name":"FairScale","subcategory":"Distributed","description":"PyTorch extension library for high performance and large scale training. This library extends basic PyTorch capabilities while adding new SOTA scaling techniques.","repositories":[{"url":"https://github.com/facebookresearch/fairscale","primary":true}]},{"category":"Optimizations","homepage_url":"http://horovod.ai/","id":"optimizations--distributed--horovod","logo_url":"http://127.0.0.1:8000/logos/a6f46dcbeb116393f35638875cc39a90f61ebc28453af34bf89ce3185da131ae.svg","name":"Horovod","subcategory":"Distributed","description":"Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet","repositories":[{"url":"https://github.com/horovod/horovod","primary":true}]}]