[{"category":"Optimizations","homepage_url":"https://depyf.readthedocs.io/en/latest/","id":"optimizations--compilers-runtimes--depyf","logo_url":"http://127.0.0.1:8000/logos/e4a063d36b8096ad076ac1c71881f1be4ccc0a18433cd7541d08de40f2c3370f.svg","name":"Depyf","subcategory":"Compilers & Runtimes","description":"depyf is a tool to help you understand debug and get insights into pytorch.compile","repositories":[{"url":"https://github.com/thuml/depyf","primary":true}]},{"category":"Optimizations","homepage_url":"https://github.com/pytorch/glow","id":"optimizations--compilers-runtimes--glow","logo_url":"http://127.0.0.1:8000/logos/b3b7299a3f02acbea42cc57cc86165e6ca112e2729f9f7c3b5284ec36b263bf8.svg","name":"Glow","subcategory":"Compilers & Runtimes","description":"Glow is a machine learning compiler and execution engine for hardware accelerators. It is designed to be used as a backend for high-level machine learning frameworks. The compiler is designed to allow state of the art compiler optimizations and code generation of neural network graphs","repositories":[{"url":"https://github.com/pytorch/glow","primary":true}]},{"category":"Optimizations","homepage_url":"https://intel.github.io/intel-extension-for-pytorch/","id":"optimizations--compilers-runtimes--intel-extension-for-pytorch","logo_url":"http://127.0.0.1:8000/logos/149058a8b3c96359ac2f000c8958b4899897602728d52d917f9335d1e0273617.svg","name":"intel-extension-for-pytorch","subcategory":"Compilers & Runtimes","description":"A Python extension to optimize performance on an Intel platform.","repositories":[{"url":"https://github.com/intel/intel-extension-for-pytorch","primary":true}]},{"category":"Optimizations","homepage_url":"https://onnxruntime.ai/","id":"optimizations--compilers-runtimes--onnx-runtime","logo_url":"http://127.0.0.1:8000/logos/a2ab7ad7ece050a1a7e0e667813b6992c1abc606d2112dd11c8c84836b5ba522.svg","name":"ONNX runtime","subcategory":"Compilers & Runtimes","description":"High performance ML inferencing and training accelerator","repositories":[{"url":"https://github.com/microsoft/onnxruntime","primary":true}]},{"category":"Optimizations","homepage_url":"https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/","id":"optimizations--compilers-runtimes--poptorch","logo_url":"http://127.0.0.1:8000/logos/941ac64f6b2ec287a2dfc558c637747c9af0d2acdcf5e6148bdbfc5e2f8a81d5.svg","name":"PopTorch","subcategory":"Compilers & Runtimes","description":"PopTorch is a set of extensions for PyTorch enabling models to be trained, evaluated and used on the Graphcore IPU.","repositories":[{"url":"https://github.com/graphcore/poptorch","primary":true}]},{"category":"Optimizations","homepage_url":"https://www.nebuly.com/","id":"optimizations--compilers-runtimes--speedster","logo_url":"http://127.0.0.1:8000/logos/ec9ec4b18b7c4f05d7caecffa57b81cf8639ba3442d8cbadba3b2bf53e806961.svg","name":"Speedster","subcategory":"Compilers & Runtimes","description":"Speedster reduces inference costs by leveraging SOTA optimization techniques that best couple your AI models with the underlying hardware (GPUs and CPUs).","repositories":[{"url":"https://github.com/nebuly-ai/optimate/tree/main/optimization/speedster","primary":true}]},{"category":"Optimizations","homepage_url":"https://pytorch.org/TensorRT/","id":"optimizations--compilers-runtimes--torch-tensorrt","logo_url":"http://127.0.0.1:8000/logos/07dbeb39c385db2b246f1fb02d2534f65455facf03f6249afc3eaf34baaddfc1.svg","name":"Torch-TensorRT","subcategory":"Compilers & Runtimes","description":"Torch-TensorRT is a inference compiler for PyTorch, targeting NVIDIA GPUs via NVIDIAâ€™s TensorRT Deep Learning Optimizer and Runtime.","repositories":[{"url":"https://github.com/RizhaoCai/PyTorch_ONNX_TensorRT","primary":true}]}]