[{"category":"Training","homepage_url":"https://adaptdl.readthedocs.io/en/latest/","id":"training--general--adaptdl","logo_url":"http://127.0.0.1:8000/logos/532512bdae901d661538ee38bb165d61918199ab2ba8f4d4cf0fddb2ed742647.svg","name":"AdaptDL","subcategory":"General","description":"AdaptDL is a resource-adaptive deep learning (DL) training and scheduling framework, and is part of the CASL open source project.","repositories":[{"url":"https://github.com/petuum/adaptdl","primary":true}]},{"category":"Training","homepage_url":"https://catalyst-team.com/","id":"training--general--catalyst","logo_url":"http://127.0.0.1:8000/logos/b8f9b0d3ee7df42d688ddfaf95b3177f256d68521167ac27b601778e339fbedc.svg","name":"Catalyst","subcategory":"General","description":"Catalyst is a PyTorch framework for Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop.","repositories":[{"url":"https://github.com/catalyst-team/catalyst","primary":true}]},{"category":"Training","homepage_url":"https://www.colossalai.org/","id":"training--general--colossalai","logo_url":"http://127.0.0.1:8000/logos/2125379104f8c98deea6e3dd3e0b2af88238792c2f41420624c1a142642c69ca.svg","name":"ColossalAI","subcategory":"General","description":"Tools to kickstart distributed training and inference","repositories":[{"url":"https://github.com/hpcaitech/ColossalAI","primary":true}]},{"category":"Training","homepage_url":"http://docs.mosaicml.com/","id":"training--general--composer","logo_url":"http://127.0.0.1:8000/logos/23206110f1d95753c27e1ebe4c63c6bc2c3b86060c4b332516cd9a3931f1900f.svg","name":"composer","subcategory":"General","description":"A open-source deep learning training library optimized for scalability and usability, integrating best practices for efficient, multi-node training","repositories":[{"url":"https://github.com/mosaicml/composer","primary":true}]},{"category":"Training","homepage_url":"https://docs.fast.ai/","id":"training--general--fastai","logo_url":"http://127.0.0.1:8000/logos/9b1d9cefe662d9c4a595cbe44fddc8c2d31a2390f6ac3ac0ae025a8c878a53ce.svg","name":"fastai","subcategory":"General","description":"fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches","repositories":[{"url":"https://github.com/fastai/fastai","primary":true}]},{"category":"Training","homepage_url":"https://github.com/microsoft/hummingbird","id":"training--general--hummingbird","logo_url":"http://127.0.0.1:8000/logos/c0d704d4e08b297aadf925661feaf0dece433d3da38e551a9b0b0d326880447f.svg","name":"Hummingbird","subcategory":"General","description":"Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch) to accelerate traditional ML models.","repositories":[{"url":"https://github.com/microsoft/hummingbird","primary":true}]},{"category":"Training","homepage_url":"https://pytorch-ignite.ai/","id":"training--general--ignite","logo_url":"http://127.0.0.1:8000/logos/35bb1388cfcd432baba2f56afef9724cf9bbeca68724e9adcb4232272ad8309f.svg","name":"Ignite","subcategory":"General","description":"High-level library to help with training and evaluating neural networks in PyTorch","repositories":[{"url":"https://github.com/pytorch/ignite","primary":true}]},{"category":"Training","homepage_url":"https://ludwig.ai/latest/","id":"training--general--ludwig","logo_url":"http://127.0.0.1:8000/logos/6e299a3f1f6df294a97064d136dd13a8078ec5ccc3d9cfdf4f3f57faf17e4b3b.svg","name":"ludwig","subcategory":"General","description":"Ludwig is a low-code framework for building custom AI models like LLMs and other deep neural networks.","repositories":[{"url":"https://github.com/ludwig-ai/ludwig","primary":true}]},{"category":"Training","homepage_url":"https://poutyne.org/","id":"training--general--poutyne","logo_url":"http://127.0.0.1:8000/logos/a291fe1f4346bc027619c3baebe8eccb7043cece79f64e2582a80056699faa44.svg","name":"Poutyne","subcategory":"General","description":"Poutyne is a simplified framework for PyTorch and handles much of the boilerplating code needed to train neural networks.","repositories":[{"url":"https://github.com/GRAAL-Research/poutyne","primary":true}]},{"category":"Training","homepage_url":"https://lightning.ai/","id":"training--general--pytorch-lightning","logo_url":"http://127.0.0.1:8000/logos/3e3486bfb4e2fa782678a99e6da800be65f30c9c9f887ef4131bfe7634797187.svg","name":"PyTorch Lightning","subcategory":"General","description":"Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.","repositories":[{"url":"https://github.com/Lightning-AI/pytorch-lightning","primary":true}]},{"category":"Training","homepage_url":"https://github.com/skorch-dev/skorch","id":"training--general--skorch","logo_url":"http://127.0.0.1:8000/logos/5fd4c8f1c65f2f5281e64ebe6b8e82bca7b16608042672804f452b8d8f2b1d98.svg","name":"skorch","subcategory":"General","description":"A scikit-learn compatible neural network library that wraps PyTorch.","repositories":[{"url":"https://github.com/skorch-dev/skorch","primary":true}]},{"category":"Training","homepage_url":"https://fidelity.github.io/stoke/","id":"training--general--stoke","logo_url":"http://127.0.0.1:8000/logos/00076432de043a325ce5d921de4d7474db9c0f85e50fdc5f48ef3bc425417831.svg","name":"stoke","subcategory":"General","description":"A lightweight wrapper for PyTorch that provides a simple declarative API for context switching between devices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU with extensions (like optimizer state sharding) by simply changing a few declarative flags.","repositories":[{"url":"https://github.com/StanfordPL/stoke","primary":true}]},{"category":"Training","homepage_url":"https://pytorch.org/data/beta/index.html","id":"training--general--torchdata","logo_url":"http://127.0.0.1:8000/logos/2d63d0eea40b76911bc64e1ea12a1520283429536287f6ed018c68dca29547b9.svg","name":"torchdata","subcategory":"General","description":"A Beta library of common modular data loading primitives for easily constructing flexible and performant data pipelines.","repositories":[{"url":"https://github.com/pytorch/data","primary":true}]},{"category":"Training","homepage_url":"https://yoshitomo-matsubara.net/torchdistill/","id":"training--general--torchdistill","logo_url":"http://127.0.0.1:8000/logos/00e6cd8399a844be633526e835a2bff2a35c27c464134204177f96b484d0b393.svg","name":"torchdistill","subcategory":"General","description":"Offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file.","repositories":[{"url":"https://github.com/yoshitomo-matsubara/torchdistill","primary":true}]},{"category":"Training","homepage_url":"https://lightning.ai/docs/torchmetrics/","id":"training--general--torchmetrics","logo_url":"http://127.0.0.1:8000/logos/fe7ab2ceb03b0c3b20725bb4810500052a76b2285265de910812dfa0abbbdf6c.svg","name":"TorchMetrics","subcategory":"General","description":"Machine learning metrics for distributed, scalable PyTorch applications.","repositories":[{"url":"https://github.com/Lightning-AI/torchmetrics","primary":true}]}]