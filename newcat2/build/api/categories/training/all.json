[{"category":"Training","homepage_url":"https://adaptdl.readthedocs.io/en/latest/","id":"training--general--adaptdl","logo_url":"http://127.0.0.1:8000/logos/532512bdae901d661538ee38bb165d61918199ab2ba8f4d4cf0fddb2ed742647.svg","name":"AdaptDL","subcategory":"General","description":"AdaptDL is a resource-adaptive deep learning (DL) training and scheduling framework, and is part of the CASL open source project.","repositories":[{"url":"https://github.com/petuum/adaptdl","primary":true}]},{"category":"Training","homepage_url":"http://avalanche.continualai.org/","id":"training--continuous-learning--avalanche","logo_url":"http://127.0.0.1:8000/logos/6db724b646af4bb6a96bf1e939a4018ee5f01db33e92af69a611275266ba90f5.svg","name":"avalanche","subcategory":"Continuous Learning","description":"Avalanche is an End-to-End Continual Learning Library based on PyTorch, for fast prototyping, training, and reproducible evaluation of continual learning algorithms.","repositories":[{"url":"https://github.com/ContinualAI/avalanche","primary":true}]},{"category":"Training","homepage_url":"https://baal.readthedocs.io/","id":"training--probabilistic-optimization--baal","logo_url":"http://127.0.0.1:8000/logos/e52fe477062933115c9eee93722f40f2dba311e6935fb12b5bf490fc94c26eb5.svg","name":"baal","subcategory":"Probabilistic & Optimization","description":"Baal is a Bayesian active learning library. Provides methods to do posterior distribution sampling in order to maximize the efficiency of labelling during active learning. Our library is suitable for research and industrial applications.","repositories":[{"url":"https://github.com/baal-org/baal","primary":true}]},{"category":"Training","homepage_url":"https://botorch.org/","id":"training--probabilistic-optimization--botorch","logo_url":"http://127.0.0.1:8000/logos/bf6001cce117af4ae13495875da21ba11ba34f5d42b525ce555f61452ad9fc6d.svg","name":"BoTorch","subcategory":"Probabilistic & Optimization","description":"A Framework for Efficient Monte-Carlo Bayesian Optimization","repositories":[{"url":"https://github.com/pytorch/botorch","primary":true}]},{"category":"Training","homepage_url":"https://catalyst-team.com/","id":"training--general--catalyst","logo_url":"http://127.0.0.1:8000/logos/b8f9b0d3ee7df42d688ddfaf95b3177f256d68521167ac27b601778e339fbedc.svg","name":"Catalyst","subcategory":"General","description":"Catalyst is a PyTorch framework for Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop.","repositories":[{"url":"https://github.com/catalyst-team/catalyst","primary":true}]},{"category":"Training","homepage_url":"https://www.colossalai.org/","id":"training--general--colossalai","logo_url":"http://127.0.0.1:8000/logos/2125379104f8c98deea6e3dd3e0b2af88238792c2f41420624c1a142642c69ca.svg","name":"ColossalAI","subcategory":"General","description":"Tools to kickstart distributed training and inference","repositories":[{"url":"https://github.com/hpcaitech/ColossalAI","primary":true}]},{"category":"Training","homepage_url":"http://docs.mosaicml.com/","id":"training--general--composer","logo_url":"http://127.0.0.1:8000/logos/23206110f1d95753c27e1ebe4c63c6bc2c3b86060c4b332516cd9a3931f1900f.svg","name":"composer","subcategory":"General","description":"A open-source deep learning training library optimized for scalability and usability, integrating best practices for efficient, multi-node training","repositories":[{"url":"https://github.com/mosaicml/composer","primary":true}]},{"category":"Training","homepage_url":"https://crypten.readthedocs.io/en/latest/","id":"training--privacy--crypten","logo_url":"http://127.0.0.1:8000/logos/a84661f05548b18ae3aa8e49e95af0fbcc12300d9482db746e87a8ab2f596531.svg","name":"CrypTen","subcategory":"Privacy","description":"CrypTen is a Privacy Preserving Machine Learning framework written using PyTorch to train models using encrypted data. It is currently not production ready and its main use is as a research framework","repositories":[{"url":"https://github.com/facebookresearch/CrypTen","primary":true}]},{"category":"Training","homepage_url":"https://docs.fast.ai/","id":"training--general--fastai","logo_url":"http://127.0.0.1:8000/logos/9b1d9cefe662d9c4a595cbe44fddc8c2d31a2390f6ac3ac0ae025a8c878a53ce.svg","name":"fastai","subcategory":"General","description":"fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches","repositories":[{"url":"https://github.com/fastai/fastai","primary":true}]},{"category":"Training","homepage_url":"https://flower.ai/","id":"training--federated-learning--flower","logo_url":"http://127.0.0.1:8000/logos/af4acf3f0922e8bb18cd7439136d299b7c8ddd7c5b8e7574bb31c17c6b4450b1.svg","name":"Flower","subcategory":"Federated Learning","description":"A unified approach to federated learning, analytics, and evaluation. Federate any workload, any ML framework, and any programming language","repositories":[{"url":"https://github.com/adap/flower","primary":true}]},{"category":"Training","homepage_url":"https://arxiv.org/abs/1809.11165","id":"training--probabilistic-optimization--gpytorch","logo_url":"http://127.0.0.1:8000/logos/edd7c89043cddc740dd6ccd3746aee9f57a590b76e93440eff93fb7272c3f4ce.svg","name":"GPyTorch","subcategory":"Probabilistic & Optimization","description":"GPyTorch is a Gaussian process library implemented using PyTorch. GPyTorch is designed for creating scalable, flexible, and modular Gaussian process models with ease","repositories":[{"url":"https://github.com/cornellius-gp/gpytorch","primary":true}]},{"category":"Training","homepage_url":"https://github.com/microsoft/hummingbird","id":"training--general--hummingbird","logo_url":"http://127.0.0.1:8000/logos/c0d704d4e08b297aadf925661feaf0dece433d3da38e551a9b0b0d326880447f.svg","name":"Hummingbird","subcategory":"General","description":"Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch) to accelerate traditional ML models.","repositories":[{"url":"https://github.com/microsoft/hummingbird","primary":true}]},{"category":"Training","homepage_url":"https://pytorch-ignite.ai/","id":"training--general--ignite","logo_url":"http://127.0.0.1:8000/logos/35bb1388cfcd432baba2f56afef9724cf9bbeca68724e9adcb4232272ad8309f.svg","name":"Ignite","subcategory":"General","description":"High-level library to help with training and evaluating neural networks in PyTorch","repositories":[{"url":"https://github.com/pytorch/ignite","primary":true}]},{"category":"Training","homepage_url":"https://ludwig.ai/latest/","id":"training--general--ludwig","logo_url":"http://127.0.0.1:8000/logos/6e299a3f1f6df294a97064d136dd13a8078ec5ccc3d9cfdf4f3f57faf17e4b3b.svg","name":"ludwig","subcategory":"General","description":"Ludwig is a low-code framework for building custom AI models like LLMs and other deep neural networks.","repositories":[{"url":"https://github.com/ludwig-ai/ludwig","primary":true}]},{"category":"Training","homepage_url":"https://opacus.ai/","id":"training--privacy--opacus","logo_url":"http://127.0.0.1:8000/logos/93e7f58529785d31148daadf57a83ea2c2f8a5881b6825df426596b79b6954ad.svg","name":"Opacus","subcategory":"Privacy","description":"Train PyTorch models with Differential Privacy","repositories":[{"url":"https://github.com/pytorch/opacus","primary":true}]},{"category":"Training","homepage_url":"https://optuna.org/","id":"training--probabilistic-optimization--optuna","logo_url":"http://127.0.0.1:8000/logos/221ccf5311fc9739ea42133e7727c4e1e3bbdb761fee27e5f71434e93ea35740.svg","name":"Optuna","subcategory":"Probabilistic & Optimization","description":"Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.","repositories":[{"url":"https://github.com/optuna/optuna","primary":true}]},{"category":"Training","homepage_url":"https://github.com/pfnet/pfrl","id":"training--reinforcement-learning--pfrl","logo_url":"http://127.0.0.1:8000/logos/e207d794eaca709d5667f9e92cefba0497d2124c413b11e863c1246dc2307b89.svg","name":"PFRL","subcategory":"Reinforcement Learning","description":"PFRL is a PyTorch-based deep reinforcement learning library","repositories":[{"url":"https://github.com/pfnet/pfrl","primary":true}]},{"category":"Training","homepage_url":"https://pomegranate.readthedocs.io/en/latest/","id":"training--probabilistic-optimization--pomegranate","logo_url":"http://127.0.0.1:8000/logos/ebf8effc430934c78a17464087fe36856c0dfe2c390f3e278beac637efb264e0.svg","name":"Pomegranate","subcategory":"Probabilistic & Optimization","description":"pomegranate is a Python package that implements fast and flexible probabilistic models ranging from individual probability distributions to compositional models such as Bayesian networks and hidden Markov models","repositories":[{"url":"https://github.com/jmschrei/pomegranate","primary":true}]},{"category":"Training","homepage_url":"https://poutyne.org/","id":"training--general--poutyne","logo_url":"http://127.0.0.1:8000/logos/a291fe1f4346bc027619c3baebe8eccb7043cece79f64e2582a80056699faa44.svg","name":"Poutyne","subcategory":"General","description":"Poutyne is a simplified framework for PyTorch and handles much of the boilerplating code needed to train neural networks.","repositories":[{"url":"https://github.com/GRAAL-Research/poutyne","primary":true}]},{"category":"Training","homepage_url":"https://pykale.github.io/","id":"training--multimodal--pykale","logo_url":"http://127.0.0.1:8000/logos/1514ccbb316a3ae5a772df0cb382bd6917c12793cc8174f0699d4fb19d1acec0.svg","name":"PyKale","subcategory":"Multimodal","description":"PyKale has a unified pipeline-based API and focuses on multimodal learning and transfer learning for graphs, images, and videos at the moment, with supporting models on deep learning and dimensionality reduction.","repositories":[{"url":"https://github.com/pykale/pykale","primary":true}]},{"category":"Training","homepage_url":"https://pyro.ai/","id":"training--probabilistic-optimization--pyro","logo_url":"http://127.0.0.1:8000/logos/ed7e2a4d5d7f216c73dc8d10febb6720c12b706bd262296605f82b5f01801d60.svg","name":"Pyro","subcategory":"Probabilistic & Optimization","description":"Deep universal probabilistic programming with Python and PyTorch","repositories":[{"url":"https://github.com/pyro-ppl/pyro","primary":true}]},{"category":"Training","homepage_url":"https://www.openmined.org/","id":"training--privacy--pysyft","logo_url":"http://127.0.0.1:8000/logos/ddcdd70695dcc0780072851c60375522b8a616c35ffdec80717cf32b87d21310.svg","name":"PySyft","subcategory":"Privacy","description":"Perform data science on data that remains in someone else's server","repositories":[{"url":"https://github.com/OpenMined/PySyft","primary":true}]},{"category":"Training","homepage_url":"https://pyg.org/","id":"training--graph--pytorch-geometric","logo_url":"http://127.0.0.1:8000/logos/5bf81a10e91e5b5336f31c52500e47e02dc3c860f3d66c7b4936161107dc12c9.svg","name":"PyTorch Geometric","subcategory":"Graph","description":"Graph Neural Network Library for PyTorch","repositories":[{"url":"https://github.com/pyg-team/pytorch_geometric","primary":true}]},{"category":"Training","homepage_url":"https://github.com/benedekrozemberczki/pytorch_geometric_temporal","id":"training--graph--pytorch-geometric-temporal","logo_url":"http://127.0.0.1:8000/logos/e178f77ad4a1b8ba7fa3e617cc445a8b47c1dd88a61972330752cc7ee6dbdd21.svg","name":"PyTorch Geometric Temporal","subcategory":"Graph","description":"PyTorch Geometric Temporal is a Spatiotemporal Signal Processing with Neural Machine Learning Models","repositories":[{"url":"https://github.com/benedekrozemberczki/pytorch_geometric_temporal","primary":true}]},{"category":"Training","homepage_url":"https://lightning.ai/","id":"training--general--pytorch-lightning","logo_url":"http://127.0.0.1:8000/logos/3e3486bfb4e2fa782678a99e6da800be65f30c9c9f887ef4131bfe7634797187.svg","name":"PyTorch Lightning","subcategory":"General","description":"Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.","repositories":[{"url":"https://github.com/Lightning-AI/pytorch-lightning","primary":true}]},{"category":"Training","homepage_url":"https://kevinmusgrave.github.io/pytorch-metric-learning/","id":"training--self-supervised--pytorch-metric-learning","logo_url":"http://127.0.0.1:8000/logos/64fe0aa1a6facd5730cee30d3904885e90f56c6cc79d9f9edda241ecf4585ff7.svg","name":"PyTorch Metric Learning","subcategory":"Self supervised","description":"Modular, flexible, and extensible library for deep metric learning.","repositories":[{"url":"https://github.com/KevinMusgrave/pytorch-metric-learning","primary":true}]},{"category":"Training","homepage_url":"https://github.com/ray-project/ray","id":"training--distributed--ray","logo_url":"http://127.0.0.1:8000/logos/3975c0f455d00dfab6b5c78281eb7c5dd6ffcf105bcc8c78764eb09a5ed534a6.svg","name":"Ray","subcategory":"Distributed","description":"Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.","repositories":[{"url":"https://github.com/ray-project/ray","primary":true}]},{"category":"Training","homepage_url":"https://renate.readthedocs.io/en/latest/","id":"training--continuous-learning--renate","logo_url":"http://127.0.0.1:8000/logos/e9fb21ed75b6d1f586d07a501eb6f349489335f0bbc79c436f625b5f0e753577.svg","name":"Renate","subcategory":"Continuous Learning","description":"Renate is a Python package for automatic retraining of neural networks models. It uses advanced Continual Learning and Lifelong Learning algorithms to achieve this purpose.","repositories":[{"url":"https://github.com/awslabs/renate","primary":true}]},{"category":"Training","homepage_url":"https://github.com/skorch-dev/skorch","id":"training--general--skorch","logo_url":"http://127.0.0.1:8000/logos/5fd4c8f1c65f2f5281e64ebe6b8e82bca7b16608042672804f452b8d8f2b1d98.svg","name":"skorch","subcategory":"General","description":"A scikit-learn compatible neural network library that wraps PyTorch.","repositories":[{"url":"https://github.com/skorch-dev/skorch","primary":true}]},{"category":"Training","homepage_url":"https://stable-baselines3.readthedocs.io/","id":"training--reinforcement-learning--stable-baselines3","logo_url":"http://127.0.0.1:8000/logos/a0f8f03828e020f203be7c6c0e19ccab88e2fe89feeaad3d517c79033bae9edf.svg","name":"Stable Baselines3","subcategory":"Reinforcement Learning","description":"Stable Baselines3 (SB3) is a set of implementations of reinforcement learning algorithms","repositories":[{"url":"https://github.com/DLR-RM/stable-baselines3","primary":true}]},{"category":"Training","homepage_url":"https://fidelity.github.io/stoke/","id":"training--general--stoke","logo_url":"http://127.0.0.1:8000/logos/00076432de043a325ce5d921de4d7474db9c0f85e50fdc5f48ef3bc425417831.svg","name":"stoke","subcategory":"General","description":"A lightweight wrapper for PyTorch that provides a simple declarative API for context switching between devices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU with extensions (like optimizer state sharding) by simply changing a few declarative flags.","repositories":[{"url":"https://github.com/StanfordPL/stoke","primary":true}]},{"category":"Training","homepage_url":"https://docs.substra.org/","id":"training--federated-learning--substra","logo_url":"http://127.0.0.1:8000/logos/57e63c3a4288f965a164da315553b98b67b369bbee4e6d9a24cdfd6cb43bc380.svg","name":"Substra","subcategory":"Federated Learning","description":"Substra is used to run complex federated learning experiments at scale.","repositories":[{"url":"https://github.com/Substra","primary":true}]},{"category":"Training","homepage_url":"https://pytorch.org/data/beta/index.html","id":"training--general--torchdata","logo_url":"http://127.0.0.1:8000/logos/2d63d0eea40b76911bc64e1ea12a1520283429536287f6ed018c68dca29547b9.svg","name":"torchdata","subcategory":"General","description":"A Beta library of common modular data loading primitives for easily constructing flexible and performant data pipelines.","repositories":[{"url":"https://github.com/pytorch/data","primary":true}]},{"category":"Training","homepage_url":"https://yoshitomo-matsubara.net/torchdistill/","id":"training--general--torchdistill","logo_url":"http://127.0.0.1:8000/logos/00e6cd8399a844be633526e835a2bff2a35c27c464134204177f96b484d0b393.svg","name":"torchdistill","subcategory":"General","description":"Offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file.","repositories":[{"url":"https://github.com/yoshitomo-matsubara/torchdistill","primary":true}]},{"category":"Training","homepage_url":"https://lightning.ai/docs/torchmetrics/","id":"training--general--torchmetrics","logo_url":"http://127.0.0.1:8000/logos/fe7ab2ceb03b0c3b20725bb4810500052a76b2285265de910812dfa0abbbdf6c.svg","name":"TorchMetrics","subcategory":"General","description":"Machine learning metrics for distributed, scalable PyTorch applications.","repositories":[{"url":"https://github.com/Lightning-AI/torchmetrics","primary":true}]},{"category":"Training","homepage_url":"https://hanruiwanghw.wixsite.com/torchquantum","id":"training--quantum--torchquantum","logo_url":"http://127.0.0.1:8000/logos/eba8a1f3cf2a73d23a295af0b6e67dcb61560ca1630acae2f929fea8cd863b90.svg","name":"TorchQuantum","subcategory":"Quantum","description":"A framework for Quantum Classical Simulation, Quantum Machine Learning, Quantum Neural Networks, Parameterized Quantum Circuits with support for easy deployments on real quantum computers.","repositories":[{"url":"https://github.com/mit-han-lab/torchquantum","primary":true}]}]