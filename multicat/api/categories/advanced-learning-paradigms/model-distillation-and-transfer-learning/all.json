[{"category":"Advanced Learning Paradigms","homepage_url":"https://pykale.github.io/","id":"advanced-learning-paradigms--model-distillation-and-transfer-learning--pykale","logo_url":"http://127.0.0.1:8000/logos/1514ccbb316a3ae5a772df0cb382bd6917c12793cc8174f0699d4fb19d1acec0.svg","name":"PyKale","subcategory":"Model Distillation and Transfer Learning","description":"PyKale has a unified pipeline-based API and focuses on multimodal learning and transfer learning for graphs, images, and videos at the moment, with supporting models on deep learning and dimensionality reduction.","repositories":[{"url":"https://github.com/pykale/pykale","primary":true}]},{"category":"Advanced Learning Paradigms","homepage_url":"https://yoshitomo-matsubara.net/torchdistill/","id":"advanced-learning-paradigms--model-distillation-and-transfer-learning--torchdistill","logo_url":"http://127.0.0.1:8000/logos/00e6cd8399a844be633526e835a2bff2a35c27c464134204177f96b484d0b393.svg","name":"torchdistill","subcategory":"Model Distillation and Transfer Learning","description":"Offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file.","repositories":[{"url":"https://github.com/yoshitomo-matsubara/torchdistill","primary":true}]}]