{"category":{"name":"Advanced Learning Paradigms","normalized_name":"advanced-learning-paradigms","subcategories":[{"name":"Model Distillation and Transfer Learning","normalized_name":"model-distillation-and-transfer-learning"}]},"foundation":"PyTorch","items":[{"category":"Advanced Learning Paradigms","id":"advanced-learning-paradigms--model-distillation-and-transfer-learning--pykale","name":"PyKale","logo":"logos/1514ccbb316a3ae5a772df0cb382bd6917c12793cc8174f0699d4fb19d1acec0.svg","subcategory":"Model Distillation and Transfer Learning","website":"https://pykale.github.io/","description":"PyKale has a unified pipeline-based API and focuses on multimodal learning and transfer learning for graphs, images, and videos at the moment, with supporting models on deep learning and dimensionality reduction.","primary_repository_url":"https://github.com/pykale/pykale"},{"category":"Advanced Learning Paradigms","id":"advanced-learning-paradigms--model-distillation-and-transfer-learning--torchdistill","name":"torchdistill","logo":"logos/00e6cd8399a844be633526e835a2bff2a35c27c464134204177f96b484d0b393.svg","subcategory":"Model Distillation and Transfer Learning","website":"https://yoshitomo-matsubara.net/torchdistill/","description":"Offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file.","primary_repository_url":"https://github.com/yoshitomo-matsubara/torchdistill"}]}