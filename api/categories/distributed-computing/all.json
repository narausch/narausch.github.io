[{"category":"Distributed Computing","homepage_url":"https://huggingface.co/docs/accelerate/index","id":"distributed-computing--performance-optimization--accelerate","logo_url":"http://127.0.0.1:8000/logos/f975c8a171af33d268543e8ec85de06980f3ddc7069f82db0645d075285c4b1d.svg","name":"accelerate","subcategory":"Performance optimization","description":"Accelerate is a library that enables the same PyTorch code to be run across distributed configurations","repositories":[{"url":"https://github.com/huggingface/accelerate","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://anomalib.readthedocs.io/en/latest/","id":"distributed-computing--training-and-evaluation--anomalib","logo_url":"http://127.0.0.1:8000/logos/79211c70ad6c0b096a678d6642e449e908438c6ba9ea651323b55b258a1a99ac.svg","name":"Anomalib","subcategory":"Training and Evaluation","description":"An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.","repositories":[{"url":"https://github.com/openvinotoolkit/anomalib","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://captum.ai/docs/introduction.html","id":"distributed-computing--training-and-evaluation--captum","logo_url":"http://127.0.0.1:8000/logos/2abc1fc7558376c329b2798f7bcde761b3644e8805d84dc631370e0e76d75b8d.svg","name":"Captum","subcategory":"Training and Evaluation","description":"An open source, extensible library for model interpretability built on PyTorch.","repositories":[{"url":"https://github.com/pytorch/captum","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://clear.ml/","id":"distributed-computing--parallelization-tools--clear-ml","logo_url":"http://127.0.0.1:8000/logos/b8f9b0d3ee7df42d688ddfaf95b3177f256d68521167ac27b601778e339fbedc.svg","name":"Clear ML","subcategory":"Parallelization tools","description":"Suite of tools to streamline your AI workflow.","repositories":[{"url":"https://github.com/allegroai/clearml","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://www.colossalai.org/","id":"distributed-computing--training-and-evaluation--colossalai","logo_url":"http://127.0.0.1:8000/logos/2125379104f8c98deea6e3dd3e0b2af88238792c2f41420624c1a142642c69ca.svg","name":"ColossalAI","subcategory":"Training and Evaluation","description":"Tools to kickstart distributed training and inference","repositories":[{"url":"https://github.com/hpcaitech/ColossalAI","primary":true}]},{"category":"Distributed Computing","homepage_url":"http://docs.mosaicml.com/","id":"distributed-computing--training-and-evaluation--composer","logo_url":"http://127.0.0.1:8000/logos/23206110f1d95753c27e1ebe4c63c6bc2c3b86060c4b332516cd9a3931f1900f.svg","name":"composer","subcategory":"Training and Evaluation","description":"A open-source deep learning training library optimized for scalability and usability, integrating best practices for efficient, multi-node training","repositories":[{"url":"https://github.com/mosaicml/composer","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://www.deepspeed.ai/","id":"distributed-computing--training-and-evaluation--deepspeed","logo_url":"http://127.0.0.1:8000/logos/6a721f2f0c7a39901db79faf3d4307b17472c558462574a3ad699666f4d0e3f3.svg","name":"DeepSpeed","subcategory":"Training and Evaluation","description":"DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.","repositories":[{"url":"https://github.com/microsoft/DeepSpeed","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://depyf.readthedocs.io/en/latest/","id":"distributed-computing--training-and-evaluation--depyf","logo_url":"http://127.0.0.1:8000/logos/e4a063d36b8096ad076ac1c71881f1be4ccc0a18433cd7541d08de40f2c3370f.svg","name":"Depyf","subcategory":"Training and Evaluation","description":"depyf is a tool to help you understand debug and get insights into pytorch.compile","repositories":[{"url":"https://github.com/thuml/depyf","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://determined.ai/","id":"distributed-computing--parallelization-tools--determined","logo_url":"http://127.0.0.1:8000/logos/6f06ce54ac2345f4d76475807a8d81d20d4f6d8f1f017ac591fb22f312ef329c.svg","name":"Determined","subcategory":"Parallelization tools","description":"Determined is an open-source machine learning platform that simplifies distributed training, hyperparameter tuning, experiment tracking, and resource management. Works with PyTorch and TensorFlow.","repositories":[{"url":"https://github.com/determined-ai/determined","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://www.dgl.ai/","id":"distributed-computing--parallelization-tools--dgl","logo_url":"http://127.0.0.1:8000/logos/8bc26a2c13a583b1695caeee24b08ead02adb0d16645e0d2763ea37305fae388.svg","name":"DGL","subcategory":"Parallelization tools","description":"Python package built to ease deep learning on graph, on top of existing DL frameworks.  Fast and memory-efficient message passing primitives for training Graph Neural Networks.","repositories":[{"url":"https://github.com/dmlc/dgl/","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://einops.rocks/","id":"distributed-computing--training-and-evaluation--einops","logo_url":"http://127.0.0.1:8000/logos/b5a90edf576208a5d90039a11703e94d01d4d4941c3bb98702a13de77d0cbec3.svg","name":"einops","subcategory":"Training and Evaluation","description":"Flexible and powerful tensor operations for readable and reliable code (for pytorch, jax, TF and others)","repositories":[{"url":"https://github.com/arogozhnikov/einops","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://ensemble-pytorch.readthedocs.io/","id":"distributed-computing--performance-optimization--ensemble-pytorch","logo_url":"http://127.0.0.1:8000/logos/1a5daa080e0d4c6242967eeac5734b744501e8d1bfe8a1f86e4305047a928ef0.svg","name":"Ensemble-Pytorch","subcategory":"Performance optimization","description":"unified ensemble framework for PyTorch to improve the performance and robustness of your ensemble based deep learning model","repositories":[{"url":"https://github.com/TorchEnsemble-Community/Ensemble-Pytorch","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://docs.fast.ai/","id":"distributed-computing--parallelization-tools--fastai","logo_url":"http://127.0.0.1:8000/logos/9b1d9cefe662d9c4a595cbe44fddc8c2d31a2390f6ac3ac0ae025a8c878a53ce.svg","name":"fastai","subcategory":"Parallelization tools","description":"fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches","repositories":[{"url":"https://github.com/fastai/fastai","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://flower.ai/","id":"distributed-computing--federated-learning--flower","logo_url":"http://127.0.0.1:8000/logos/af4acf3f0922e8bb18cd7439136d299b7c8ddd7c5b8e7574bb31c17c6b4450b1.svg","name":"Flower","subcategory":"Federated learning","description":"A unified approach to federated learning, analytics, and evaluation. Federate any workload, any ML framework, and any programming language","repositories":[{"url":"https://github.com/adap/flower","primary":true}]},{"category":"Distributed Computing","homepage_url":"http://horovod.ai/","id":"distributed-computing--training-and-evaluation--horovod","logo_url":"http://127.0.0.1:8000/logos/a6f46dcbeb116393f35638875cc39a90f61ebc28453af34bf89ce3185da131ae.svg","name":"Horovod","subcategory":"Training and Evaluation","description":"Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet","repositories":[{"url":"https://github.com/horovod/horovod","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://pytorch-ignite.ai/","id":"distributed-computing--training-and-evaluation--ignite","logo_url":"http://127.0.0.1:8000/logos/35bb1388cfcd432baba2f56afef9724cf9bbeca68724e9adcb4232272ad8309f.svg","name":"Ignite","subcategory":"Training and Evaluation","description":"High-level library to help with training and evaluating neural networks in PyTorch","repositories":[{"url":"https://github.com/pytorch/ignite","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://intel.github.io/intel-extension-for-pytorch/","id":"distributed-computing--performance-optimization--intel-extension-for-pytorch","logo_url":"http://127.0.0.1:8000/logos/149058a8b3c96359ac2f000c8958b4899897602728d52d917f9335d1e0273617.svg","name":"intel-extension-for-pytorch","subcategory":"Performance optimization","description":"A Python extension to optimize performance on an Intel platform.","repositories":[{"url":"https://github.com/intel/intel-extension-for-pytorch","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://ivy.dev/","id":"distributed-computing--parallelization-tools--ivy","logo_url":"http://127.0.0.1:8000/logos/4ac7177a12ab6aa7b56c2420367cbe3b6e1be0de2f9753d69198f5ca9a7731bb.svg","name":"ivy","subcategory":"Parallelization tools","description":"Convert Machine Learning Code Between Frameworks","repositories":[{"url":"https://github.com/ivy-llc/ivy","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://mmf.sh/","id":"distributed-computing--training-and-evaluation--mmf","logo_url":"http://127.0.0.1:8000/logos/0c6791d31524bbcc5d00788ef39db3ff6576fdd3d3b1f8cea530d5c551e9098f.svg","name":"MMF","subcategory":"Training and Evaluation","description":"A modular framework for vision & language multimodal research.","repositories":[{"url":"https://github.com/facebookresearch/mmf","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://intel.github.io/neural-compressor/latest/docs/source/Welcome.html","id":"distributed-computing--performance-optimization--neural-compressor","logo_url":"http://127.0.0.1:8000/logos/ba40986b1563b5634522df1bb286e54e543fea3a4ce7e5d0f72c2de633c074f5.svg","name":"neural-compressor","subcategory":"Performance optimization","description":"An open-source Python library supporting popular model compression techniques on all deep learning frameworks.","repositories":[{"url":"https://github.com/intel/neural-compressor","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://github.com/octoml/octoml-profile","id":"distributed-computing--parallelization-tools--octoml-profiler","logo_url":"http://127.0.0.1:8000/logos/6a0430bf99ada8d6b9a2f7c296d8719bd6d5081f40036783c1ee51c58ae73217.svg","name":"OctoML Profiler","subcategory":"Parallelization tools","description":"octoml-profile is a python library and cloud service that enables ML engineers to easily assess the performance and cost of PyTorch models on cloud hardware with state-of-the-art ML acceleration technology.","repositories":[{"url":"https://github.com/octoml/octoml-profile","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://onnxruntime.ai/","id":"distributed-computing--training-and-evaluation--onnx-runtime","logo_url":"http://127.0.0.1:8000/logos/a2ab7ad7ece050a1a7e0e667813b6992c1abc606d2112dd11c8c84836b5ba522.svg","name":"ONNX runtime","subcategory":"Training and Evaluation","description":"High performance ML inferencing and training accelerator","repositories":[{"url":"https://github.com/microsoft/onnxruntime","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://opacus.ai/","id":"distributed-computing--training-and-evaluation--opacus","logo_url":"http://127.0.0.1:8000/logos/93e7f58529785d31148daadf57a83ea2c2f8a5881b6825df426596b79b6954ad.svg","name":"Opacus","subcategory":"Training and Evaluation","description":"Train PyTorch models with Differential Privacy","repositories":[{"url":"https://github.com/pytorch/opacus","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://opencompass.org.cn/home","id":"distributed-computing--training-and-evaluation--opencompass","logo_url":"http://127.0.0.1:8000/logos/7a70f2cbec75cbed6adfa360674d53e5fd6536ee1b342b52588a3dbd35ef8860.svg","name":"OpenCompass","subcategory":"Training and Evaluation","description":"OpenCompass is an LLM evaluation platform, supporting a wide range of models (Llama3, Mistral, InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.","repositories":[{"url":"https://github.com/open-compass/opencompass","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://optuna.org/","id":"distributed-computing--performance-optimization--optuna","logo_url":"http://127.0.0.1:8000/logos/221ccf5311fc9739ea42133e7727c4e1e3bbdb761fee27e5f71434e93ea35740.svg","name":"Optuna","subcategory":"Performance optimization","description":"Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.","repositories":[{"url":"https://github.com/optuna/optuna","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/","id":"distributed-computing--performance-optimization--poptorch","logo_url":"http://127.0.0.1:8000/logos/941ac64f6b2ec287a2dfc558c637747c9af0d2acdcf5e6148bdbfc5e2f8a81d5.svg","name":"PopTorch","subcategory":"Performance optimization","description":"PopTorch is a set of extensions for PyTorch enabling models to be trained, evaluated and used on the Graphcore IPU.","repositories":[{"url":"https://github.com/graphcore/poptorch","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://www.openmined.org/","id":"distributed-computing--parallelization-tools--pysyft","logo_url":"http://127.0.0.1:8000/logos/ddcdd70695dcc0780072851c60375522b8a616c35ffdec80717cf32b87d21310.svg","name":"PySyft","subcategory":"Parallelization tools","description":"Perform data science on data that remains in someone else's server","repositories":[{"url":"https://github.com/OpenMined/PySyft","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://lightning.ai/","id":"distributed-computing--performance-optimization--pytorch-lightning","logo_url":"http://127.0.0.1:8000/logos/9ea7a73b721cef9519fd2ac3cc48b9dd167e210bda4ce34ae8c8984d9ad28ebd.svg","name":"PyTorch Lightning","subcategory":"Performance optimization","description":"Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.","repositories":[{"url":"https://github.com/Lightning-AI/pytorch-lightning","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://github.com/ray-project/ray","id":"distributed-computing--performance-optimization--ray","logo_url":"http://127.0.0.1:8000/logos/3975c0f455d00dfab6b5c78281eb7c5dd6ffcf105bcc8c78764eb09a5ed534a6.svg","name":"Ray","subcategory":"Performance optimization","description":"Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.","repositories":[{"url":"https://github.com/ray-project/ray","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://github.com/skorch-dev/skorch","id":"distributed-computing--training-and-evaluation--skorch","logo_url":"http://127.0.0.1:8000/logos/5fd4c8f1c65f2f5281e64ebe6b8e82bca7b16608042672804f452b8d8f2b1d98.svg","name":"skorch","subcategory":"Training and Evaluation","description":"A scikit-learn compatible neural network library that wraps PyTorch.","repositories":[{"url":"https://github.com/skorch-dev/skorch","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://docs.substra.org/","id":"distributed-computing--federated-learning--substra","logo_url":"http://127.0.0.1:8000/logos/57e63c3a4288f965a164da315553b98b67b369bbee4e6d9a24cdfd6cb43bc380.svg","name":"Substra","subcategory":"Federated learning","description":"Substra is used to run complex federated learning experiments at scale.","repositories":[{"url":"https://github.com/Substra","primary":true}]},{"category":"Distributed Computing","homepage_url":"http://tensorly.org/","id":"distributed-computing--training-and-evaluation--tensorly","logo_url":"http://127.0.0.1:8000/logos/d4d2be7563e5a626bf144163697a53e71a99556dc4fdeb8b614bdc7a24793a6e.svg","name":"TensorLy","subcategory":"Training and Evaluation","description":"TensorLy is a Python library that aims at making tensor learning simple and accessible. It allows to easily perform tensor decomposition, tensor learning and tensor algebra. Its backend system allows to seamlessly perform computation with NumPy, PyTorch, JAX, TensorFlow, CuPy or Paddle, and run methods at scale on CPU or GPU.","repositories":[{"url":"https://github.com/tensorly/tensorly","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://pytorch.org/TensorRT/","id":"distributed-computing--performance-optimization--torch-tensorrt","logo_url":"http://127.0.0.1:8000/logos/07dbeb39c385db2b246f1fb02d2534f65455facf03f6249afc3eaf34baaddfc1.svg","name":"Torch-TensorRT","subcategory":"Performance optimization","description":"Torch-TensorRT is a inference compiler for PyTorch, targeting NVIDIA GPUs via NVIDIA’s TensorRT Deep Learning Optimizer and Runtime.","repositories":[{"url":"https://github.com/RizhaoCai/PyTorch_ONNX_TensorRT","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://yoshitomo-matsubara.net/torchdistill/","id":"distributed-computing--training-and-evaluation--torchdistill","logo_url":"http://127.0.0.1:8000/logos/00e6cd8399a844be633526e835a2bff2a35c27c464134204177f96b484d0b393.svg","name":"torchdistill","subcategory":"Training and Evaluation","description":"Offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file.","repositories":[{"url":"https://github.com/yoshitomo-matsubara/torchdistill","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://lightning.ai/docs/torchmetrics/","id":"distributed-computing--training-and-evaluation--torchmetrics","logo_url":"http://127.0.0.1:8000/logos/fe7ab2ceb03b0c3b20725bb4810500052a76b2285265de910812dfa0abbbdf6c.svg","name":"TorchMetrics","subcategory":"Training and Evaluation","description":"Machine learning metrics for distributed, scalable PyTorch applications.","repositories":[{"url":"https://github.com/Lightning-AI/torchmetrics","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://torchopt.readthedocs.io/en/latest/","id":"distributed-computing--performance-optimization--torchopt","logo_url":"http://127.0.0.1:8000/logos/fa7428c96683f978432a500f84b389bd2f5a07cb9738885282a728a9015c5cfd.svg","name":"Torchopt","subcategory":"Performance optimization","description":"An efficient library for differentiable optimization built upon PyTorch","repositories":[{"url":"https://github.com/metaopt/TorchOpt","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://usb.readthedocs.io/en/main/","id":"distributed-computing--training-and-evaluation--usb","logo_url":"http://127.0.0.1:8000/logos/ba2f979d55cfe3b448250c198b2c08437accb672f3f21600f0b3c979a399c9e7.svg","name":"USB","subcategory":"Training and Evaluation","description":"Benchmark tool for developing and evaluating Semi-suprevised learning algorithms.  Includes an implementation of 14 SSL algorithms and 15 tasks for evaluation for CV, NLP, and Audio","repositories":[{"url":"https://github.com/microsoft/Semi-supervised-learning","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://ml.energy/zeus/","id":"distributed-computing--performance-optimization--zeus","logo_url":"http://127.0.0.1:8000/logos/88fe3408a97b0da3dfbd5328126047c69b1627ea7655a2379de2169c0dc701c2.svg","name":"Zeus","subcategory":"Performance optimization","description":"Zeus is a library for measuring the energy consumption of Deep Learning workloads and optimizing their energy consumption.","repositories":[{"url":"https://github.com/ml-energy/zeus","primary":true}]}]