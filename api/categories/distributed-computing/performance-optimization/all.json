[{"category":"Distributed Computing","homepage_url":"https://huggingface.co/docs/accelerate/index","id":"distributed-computing--performance-optimization--accelerate","logo_url":"http://127.0.0.1:8000/logos/f975c8a171af33d268543e8ec85de06980f3ddc7069f82db0645d075285c4b1d.svg","name":"accelerate","subcategory":"Performance optimization","description":"Accelerate is a library that enables the same PyTorch code to be run across distributed configurations","repositories":[{"url":"https://github.com/huggingface/accelerate","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://ensemble-pytorch.readthedocs.io/","id":"distributed-computing--performance-optimization--ensemble-pytorch","logo_url":"http://127.0.0.1:8000/logos/1a5daa080e0d4c6242967eeac5734b744501e8d1bfe8a1f86e4305047a928ef0.svg","name":"Ensemble-Pytorch","subcategory":"Performance optimization","description":"unified ensemble framework for PyTorch to improve the performance and robustness of your ensemble based deep learning model","repositories":[{"url":"https://github.com/TorchEnsemble-Community/Ensemble-Pytorch","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://intel.github.io/intel-extension-for-pytorch/","id":"distributed-computing--performance-optimization--intel-extension-for-pytorch","logo_url":"http://127.0.0.1:8000/logos/149058a8b3c96359ac2f000c8958b4899897602728d52d917f9335d1e0273617.svg","name":"intel-extension-for-pytorch","subcategory":"Performance optimization","description":"A Python extension to optimize performance on an Intel platform.","repositories":[{"url":"https://github.com/intel/intel-extension-for-pytorch","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://intel.github.io/neural-compressor/latest/docs/source/Welcome.html","id":"distributed-computing--performance-optimization--neural-compressor","logo_url":"http://127.0.0.1:8000/logos/ba40986b1563b5634522df1bb286e54e543fea3a4ce7e5d0f72c2de633c074f5.svg","name":"neural-compressor","subcategory":"Performance optimization","description":"An open-source Python library supporting popular model compression techniques on all deep learning frameworks.","repositories":[{"url":"https://github.com/intel/neural-compressor","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://optuna.org/","id":"distributed-computing--performance-optimization--optuna","logo_url":"http://127.0.0.1:8000/logos/221ccf5311fc9739ea42133e7727c4e1e3bbdb761fee27e5f71434e93ea35740.svg","name":"Optuna","subcategory":"Performance optimization","description":"Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.","repositories":[{"url":"https://github.com/optuna/optuna","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/","id":"distributed-computing--performance-optimization--poptorch","logo_url":"http://127.0.0.1:8000/logos/941ac64f6b2ec287a2dfc558c637747c9af0d2acdcf5e6148bdbfc5e2f8a81d5.svg","name":"PopTorch","subcategory":"Performance optimization","description":"PopTorch is a set of extensions for PyTorch enabling models to be trained, evaluated and used on the Graphcore IPU.","repositories":[{"url":"https://github.com/graphcore/poptorch","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://lightning.ai/","id":"distributed-computing--performance-optimization--pytorch-lightning","logo_url":"http://127.0.0.1:8000/logos/9ea7a73b721cef9519fd2ac3cc48b9dd167e210bda4ce34ae8c8984d9ad28ebd.svg","name":"PyTorch Lightning","subcategory":"Performance optimization","description":"Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.","repositories":[{"url":"https://github.com/Lightning-AI/pytorch-lightning","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://github.com/ray-project/ray","id":"distributed-computing--performance-optimization--ray","logo_url":"http://127.0.0.1:8000/logos/3975c0f455d00dfab6b5c78281eb7c5dd6ffcf105bcc8c78764eb09a5ed534a6.svg","name":"Ray","subcategory":"Performance optimization","description":"Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.","repositories":[{"url":"https://github.com/ray-project/ray","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://pytorch.org/TensorRT/","id":"distributed-computing--performance-optimization--torch-tensorrt","logo_url":"http://127.0.0.1:8000/logos/07dbeb39c385db2b246f1fb02d2534f65455facf03f6249afc3eaf34baaddfc1.svg","name":"Torch-TensorRT","subcategory":"Performance optimization","description":"Torch-TensorRT is a inference compiler for PyTorch, targeting NVIDIA GPUs via NVIDIAâ€™s TensorRT Deep Learning Optimizer and Runtime.","repositories":[{"url":"https://github.com/RizhaoCai/PyTorch_ONNX_TensorRT","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://torchopt.readthedocs.io/en/latest/","id":"distributed-computing--performance-optimization--torchopt","logo_url":"http://127.0.0.1:8000/logos/fa7428c96683f978432a500f84b389bd2f5a07cb9738885282a728a9015c5cfd.svg","name":"Torchopt","subcategory":"Performance optimization","description":"An efficient library for differentiable optimization built upon PyTorch","repositories":[{"url":"https://github.com/metaopt/TorchOpt","primary":true}]},{"category":"Distributed Computing","homepage_url":"https://ml.energy/zeus/","id":"distributed-computing--performance-optimization--zeus","logo_url":"http://127.0.0.1:8000/logos/88fe3408a97b0da3dfbd5328126047c69b1627ea7655a2379de2169c0dc701c2.svg","name":"Zeus","subcategory":"Performance optimization","description":"Zeus is a library for measuring the energy consumption of Deep Learning workloads and optimizing their energy consumption.","repositories":[{"url":"https://github.com/ml-energy/zeus","primary":true}]}]