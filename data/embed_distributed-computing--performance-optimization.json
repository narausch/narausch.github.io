{"category":{"name":"Distributed Computing","normalized_name":"distributed-computing","subcategories":[{"name":"Performance optimization","normalized_name":"performance-optimization"}]},"foundation":"PyTorch","items":[{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--accelerate","name":"accelerate","logo":"logos/f975c8a171af33d268543e8ec85de06980f3ddc7069f82db0645d075285c4b1d.svg","subcategory":"Performance optimization","website":"https://huggingface.co/docs/accelerate/index","description":"Accelerate is a library that enables the same PyTorch code to be run across distributed configurations","primary_repository_url":"https://github.com/huggingface/accelerate"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--ensemble-pytorch","name":"Ensemble-Pytorch","logo":"logos/1a5daa080e0d4c6242967eeac5734b744501e8d1bfe8a1f86e4305047a928ef0.svg","subcategory":"Performance optimization","website":"https://ensemble-pytorch.readthedocs.io/","description":"unified ensemble framework for PyTorch to improve the performance and robustness of your ensemble based deep learning model","primary_repository_url":"https://github.com/TorchEnsemble-Community/Ensemble-Pytorch"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--intel-extension-for-pytorch","name":"intel-extension-for-pytorch","logo":"logos/149058a8b3c96359ac2f000c8958b4899897602728d52d917f9335d1e0273617.svg","subcategory":"Performance optimization","website":"https://intel.github.io/intel-extension-for-pytorch/","description":"A Python extension to optimize performance on an Intel platform.","primary_repository_url":"https://github.com/intel/intel-extension-for-pytorch"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--neural-compressor","name":"neural-compressor","logo":"logos/ba40986b1563b5634522df1bb286e54e543fea3a4ce7e5d0f72c2de633c074f5.svg","subcategory":"Performance optimization","website":"https://intel.github.io/neural-compressor/latest/docs/source/Welcome.html","description":"An open-source Python library supporting popular model compression techniques on all deep learning frameworks.","primary_repository_url":"https://github.com/intel/neural-compressor"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--optuna","name":"Optuna","logo":"logos/221ccf5311fc9739ea42133e7727c4e1e3bbdb761fee27e5f71434e93ea35740.svg","subcategory":"Performance optimization","website":"https://optuna.org/","description":"Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.","primary_repository_url":"https://github.com/optuna/optuna"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--poptorch","name":"PopTorch","logo":"logos/941ac64f6b2ec287a2dfc558c637747c9af0d2acdcf5e6148bdbfc5e2f8a81d5.svg","subcategory":"Performance optimization","website":"https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/","description":"PopTorch is a set of extensions for PyTorch enabling models to be trained, evaluated and used on the Graphcore IPU.","primary_repository_url":"https://github.com/graphcore/poptorch"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--pytorch-lightning","name":"PyTorch Lightning","logo":"logos/9ea7a73b721cef9519fd2ac3cc48b9dd167e210bda4ce34ae8c8984d9ad28ebd.svg","subcategory":"Performance optimization","website":"https://lightning.ai/","description":"Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.","primary_repository_url":"https://github.com/Lightning-AI/pytorch-lightning"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--ray","name":"Ray","logo":"logos/3975c0f455d00dfab6b5c78281eb7c5dd6ffcf105bcc8c78764eb09a5ed534a6.svg","subcategory":"Performance optimization","website":"https://github.com/ray-project/ray","description":"Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.","primary_repository_url":"https://github.com/ray-project/ray"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--torch-tensorrt","name":"Torch-TensorRT","logo":"logos/07dbeb39c385db2b246f1fb02d2534f65455facf03f6249afc3eaf34baaddfc1.svg","subcategory":"Performance optimization","website":"https://pytorch.org/TensorRT/","description":"Torch-TensorRT is a inference compiler for PyTorch, targeting NVIDIA GPUs via NVIDIAâ€™s TensorRT Deep Learning Optimizer and Runtime.","primary_repository_url":"https://github.com/RizhaoCai/PyTorch_ONNX_TensorRT"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--torchopt","name":"Torchopt","logo":"logos/fa7428c96683f978432a500f84b389bd2f5a07cb9738885282a728a9015c5cfd.svg","subcategory":"Performance optimization","website":"https://torchopt.readthedocs.io/en/latest/","description":"An efficient library for differentiable optimization built upon PyTorch","primary_repository_url":"https://github.com/metaopt/TorchOpt"},{"category":"Distributed Computing","id":"distributed-computing--performance-optimization--zeus","name":"Zeus","logo":"logos/88fe3408a97b0da3dfbd5328126047c69b1627ea7655a2379de2169c0dc701c2.svg","subcategory":"Performance optimization","website":"https://ml.energy/zeus/","description":"Zeus is a library for measuring the energy consumption of Deep Learning workloads and optimizing their energy consumption.","primary_repository_url":"https://github.com/ml-energy/zeus"}]}