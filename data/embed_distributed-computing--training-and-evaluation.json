{"category":{"name":"Distributed Computing","normalized_name":"distributed-computing","subcategories":[{"name":"Training and Evaluation","normalized_name":"training-and-evaluation"}]},"foundation":"PyTorch","items":[{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--anomalib","name":"Anomalib","logo":"logos/79211c70ad6c0b096a678d6642e449e908438c6ba9ea651323b55b258a1a99ac.svg","subcategory":"Training and Evaluation","website":"https://anomalib.readthedocs.io/en/latest/","description":"An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.","primary_repository_url":"https://github.com/openvinotoolkit/anomalib"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--captum","name":"Captum","logo":"logos/2abc1fc7558376c329b2798f7bcde761b3644e8805d84dc631370e0e76d75b8d.svg","subcategory":"Training and Evaluation","website":"https://captum.ai/docs/introduction.html","description":"An open source, extensible library for model interpretability built on PyTorch.","primary_repository_url":"https://github.com/pytorch/captum"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--colossalai","name":"ColossalAI","logo":"logos/2125379104f8c98deea6e3dd3e0b2af88238792c2f41420624c1a142642c69ca.svg","subcategory":"Training and Evaluation","website":"https://www.colossalai.org/","description":"Tools to kickstart distributed training and inference","primary_repository_url":"https://github.com/hpcaitech/ColossalAI"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--composer","name":"composer","logo":"logos/23206110f1d95753c27e1ebe4c63c6bc2c3b86060c4b332516cd9a3931f1900f.svg","subcategory":"Training and Evaluation","website":"http://docs.mosaicml.com/","description":"A open-source deep learning training library optimized for scalability and usability, integrating best practices for efficient, multi-node training","primary_repository_url":"https://github.com/mosaicml/composer"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--deepspeed","name":"DeepSpeed","logo":"logos/6a721f2f0c7a39901db79faf3d4307b17472c558462574a3ad699666f4d0e3f3.svg","subcategory":"Training and Evaluation","website":"https://www.deepspeed.ai/","description":"DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.","primary_repository_url":"https://github.com/microsoft/DeepSpeed"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--depyf","name":"Depyf","logo":"logos/e4a063d36b8096ad076ac1c71881f1be4ccc0a18433cd7541d08de40f2c3370f.svg","subcategory":"Training and Evaluation","website":"https://depyf.readthedocs.io/en/latest/","description":"depyf is a tool to help you understand debug and get insights into pytorch.compile","primary_repository_url":"https://github.com/thuml/depyf"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--einops","name":"einops","logo":"logos/6b14333eb730fab0144979bb8029431b0c677e02dfd2944f381b7e96e31054ce.svg","subcategory":"Training and Evaluation","website":"https://einops.rocks/","description":"Flexible and powerful tensor operations for readable and reliable code (for pytorch, jax, TF and others)","primary_repository_url":"https://github.com/arogozhnikov/einops"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--horovod","name":"Horovod","logo":"logos/a6f46dcbeb116393f35638875cc39a90f61ebc28453af34bf89ce3185da131ae.svg","subcategory":"Training and Evaluation","website":"http://horovod.ai/","description":"Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet","primary_repository_url":"https://github.com/horovod/horovod"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--ignite","name":"Ignite","logo":"logos/35bb1388cfcd432baba2f56afef9724cf9bbeca68724e9adcb4232272ad8309f.svg","subcategory":"Training and Evaluation","website":"https://pytorch-ignite.ai/","description":"High-level library to help with training and evaluating neural networks in PyTorch","primary_repository_url":"https://github.com/pytorch/ignite"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--mmf","name":"MMF","logo":"logos/0c6791d31524bbcc5d00788ef39db3ff6576fdd3d3b1f8cea530d5c551e9098f.svg","subcategory":"Training and Evaluation","website":"https://mmf.sh/","description":"A modular framework for vision & language multimodal research.","primary_repository_url":"https://github.com/facebookresearch/mmf"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--onnx-runtime","name":"ONNX runtime","logo":"logos/a2ab7ad7ece050a1a7e0e667813b6992c1abc606d2112dd11c8c84836b5ba522.svg","subcategory":"Training and Evaluation","website":"https://onnxruntime.ai/","description":"High performance ML inferencing and training accelerator","primary_repository_url":"https://github.com/microsoft/onnxruntime"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--opacus","name":"Opacus","logo":"logos/93e7f58529785d31148daadf57a83ea2c2f8a5881b6825df426596b79b6954ad.svg","subcategory":"Training and Evaluation","website":"https://opacus.ai/","description":"Train PyTorch models with Differential Privacy","primary_repository_url":"https://github.com/pytorch/opacus"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--opencompass","name":"OpenCompass","logo":"logos/7a70f2cbec75cbed6adfa360674d53e5fd6536ee1b342b52588a3dbd35ef8860.svg","subcategory":"Training and Evaluation","website":"https://opencompass.org.cn/home","description":"OpenCompass is an LLM evaluation platform, supporting a wide range of models (Llama3, Mistral, InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.","primary_repository_url":"https://github.com/open-compass/opencompass"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--skorch","name":"skorch","logo":"logos/5fd4c8f1c65f2f5281e64ebe6b8e82bca7b16608042672804f452b8d8f2b1d98.svg","subcategory":"Training and Evaluation","website":"https://github.com/skorch-dev/skorch","description":"A scikit-learn compatible neural network library that wraps PyTorch.","primary_repository_url":"https://github.com/skorch-dev/skorch"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--tensorly","name":"TensorLy","logo":"logos/d4d2be7563e5a626bf144163697a53e71a99556dc4fdeb8b614bdc7a24793a6e.svg","subcategory":"Training and Evaluation","website":"http://tensorly.org/","description":"TensorLy is a Python library that aims at making tensor learning simple and accessible. It allows to easily perform tensor decomposition, tensor learning and tensor algebra. Its backend system allows to seamlessly perform computation with NumPy, PyTorch, JAX, TensorFlow, CuPy or Paddle, and run methods at scale on CPU or GPU.","primary_repository_url":"https://github.com/tensorly/tensorly"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--torchdistill","name":"torchdistill","logo":"logos/00e6cd8399a844be633526e835a2bff2a35c27c464134204177f96b484d0b393.svg","subcategory":"Training and Evaluation","website":"https://yoshitomo-matsubara.net/torchdistill/","description":"Offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file.","primary_repository_url":"https://github.com/yoshitomo-matsubara/torchdistill"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--torchmetrics","name":"TorchMetrics","logo":"logos/fe7ab2ceb03b0c3b20725bb4810500052a76b2285265de910812dfa0abbbdf6c.svg","subcategory":"Training and Evaluation","website":"https://lightning.ai/docs/torchmetrics/","description":"Machine learning metrics for distributed, scalable PyTorch applications.","primary_repository_url":"https://github.com/Lightning-AI/torchmetrics"},{"category":"Distributed Computing","id":"distributed-computing--training-and-evaluation--usb","name":"USB","logo":"logos/ba2f979d55cfe3b448250c198b2c08437accb672f3f21600f0b3c979a399c9e7.svg","subcategory":"Training and Evaluation","website":"https://usb.readthedocs.io/en/main/","description":"Benchmark tool for developing and evaluating Semi-suprevised learning algorithms.  Includes an implementation of 14 SSL algorithms and 15 tasks for evaluation for CV, NLP, and Audio","primary_repository_url":"https://github.com/microsoft/Semi-supervised-learning"}]}